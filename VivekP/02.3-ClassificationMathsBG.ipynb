{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "In this document, we provide an introduction to the mathematics behind classification problems. In classification, we seek to make predictions about the class $Y$ of an object, using features $\\boldsymbol{x}$. The classes take values in a discrete set $\\mathcal{J}$. Since we make predictions in $\\mathcal{J}$, the use of the squared error from regression is not appropriate. We therefore introduce a non-negative loss function $L = L(Y, \\widehat{Y}(\\boldsymbol{x}))$ and aim to find a function $\\widehat{Y}(\\boldsymbol{x})$ that minimizes the expected loss (risk) $R^*$:\n",
    "\n",
    "$$\n",
    "R^*(Y, \\widehat{Y}(\\boldsymbol{x})) = \\mathbb{E}_{\\boldsymbol{x}, Y}[L(Y, \\widehat{Y}(\\boldsymbol{x}))],\n",
    "$$\n",
    "\n",
    "where $L(Y, \\widehat{Y}(\\boldsymbol{x}))$ is the loss incurred when predicting $\\widehat{Y}(\\boldsymbol{x})$ when the true value is $Y$. A popular choice of loss function is 0/1 loss, $L_{0/1}$:\n",
    "\n",
    "$$\n",
    "L_{0/1}(Y, \\widehat{Y}(\\boldsymbol{x})) = \\mathbb{I}(Y \\neq \\widehat{Y}(\\boldsymbol{x})),\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}$ is the indicator function. The corresponding risk is:\n",
    "\n",
    "$$\n",
    "R^*_{0/1}(Y, \\widehat{Y}(\\boldsymbol{x})) = \\Pr(Y \\neq \\widehat{Y}(\\boldsymbol{x})) = 1 - \\Pr(Y = \\widehat{Y}(\\boldsymbol{x})).\n",
    "$$\n",
    "\n",
    "The optimal classifier minimizing $R^*_{0/1}$ is called the Bayes classifier. Using the expression above, we can see that it takes the form:\n",
    "\n",
    "$$\n",
    "\\widehat{Y}_{\\text{Bayes}}(\\boldsymbol{x}) = \\arg \\min_{j \\in \\mathcal{Y}} R^*_{0/1}(Y, \\widehat{Y}(\\boldsymbol{x})) = \\arg \\max_{j \\in \\mathcal{Y}} \\Pr(Y=j \\mid \\boldsymbol{x}).\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem, we can write $\\Pr(Y=j \\mid \\boldsymbol{x}) = \\frac{\\Pr(Y=j)f_j(\\boldsymbol{x})}{\\sum_{k \\in \\mathcal{J}} \\Pr(Y=k)f_k(\\boldsymbol{x})}$. The denominator does not depend on $j$, and therefore we have \n",
    "\n",
    "$$\n",
    "\\widehat{Y}_{\\text{Bayes}}(\\boldsymbol{x}) = \\arg \\max_{j \\in \\mathcal{Y}} \\Pr(Y=j) f_j(\\boldsymbol{x}),\n",
    "$$\n",
    "\n",
    "and after applying a log-transformation:\n",
    "\n",
    "$$\n",
    "\\widehat{Y}_{\\text{Bayes}}(\\boldsymbol{x}) = \\arg \\max_{j \\in \\mathcal{Y}} \\left\\{ \\log \\Pr(Y=j) + \\log f_j(\\boldsymbol{x}) \\right\\}.\n",
    "$$\n",
    "\n",
    "However, solving this problem to obtain the Bayes classifier is computationally difficult, as the distribution $\\Pr$ and the conditional densities $f_j$ belong to a large class of functions. In practice, we would use a dataset $L = \\{(\\boldsymbol{x}_1, Y_1), \\dots, (\\boldsymbol{x}_N, Y_N)\\}$ with $N$ data points and attempt to minimize the empirical risk:\n",
    "\n",
    "$$\n",
    "R_{0/1}(Y, \\widehat{Y}(\\boldsymbol{x})) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(Y_i \\neq \\widehat{Y}(\\boldsymbol{x}_i)).\n",
    "$$\n",
    "\n",
    "Many classification methods aim to approximate the Bayes classifier by minimizing the above risk. In particular, we get parametric methods, which assume a parametric class of functions for $f_j$, and non-parametric methods which do not make any assumptions on $f_j$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
