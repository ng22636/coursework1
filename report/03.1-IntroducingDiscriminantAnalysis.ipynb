{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92543d8a-9726-4a7a-a2c8-eae707e6e35a",
   "metadata": {},
   "source": [
    "# Discriminant Analysis\n",
    "\n",
    "### Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Decision Rule Formulation](#Decision-Rule-Formulation)\n",
    "3. [Linear Discriminant Analysis](#Linear-Discriminant-Analysis)\n",
    "4. [Quadratic Discriminant Analysis](#Quadratic-Discriminant-Analysis)\n",
    "5. [Comparison of LDA and QDA](#Comparison-of-LDA-and-QDA)\n",
    "6. [Regularised Discriminant Analysis](#Regularised-Discriminant-Analysis)\n",
    "7. [Evaluation of Discriminant Analysis](#Evaluation-of-Discriminant-Analysis)\n",
    "8. [Conclusion](#Conclusion)\n",
    "9. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418389e-240a-4953-922b-0a7bd8447720",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Discriminant Analysis is a technique widely used in classification. Applications include medical research, finance and ecology. There are a number of different methods which correspond to this type of analysis, each depending on their own assumptions about the data or parameters. We will focus on linear discriminant analysis (LDA), quadratic discriminant analysis (QDA) and regularised discriminant analysis (RDA).\n",
    "\n",
    "## Decision Rule Formulation\n",
    "First, we note that all discriminant classification method requires a decision rule:\n",
    "$$\\hat{G}^*(x) \\in \\arg \\max_{k} \\mathbb{P}(G = k | X = x)$$\n",
    "\n",
    "Here, $\\hat{G}^*(x)$ is the optimal class we have labelled the given data as, with $G=k$ the possible class labels and $X=x$ the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18697199-7864-42b4-9e6e-78664b14e5da",
   "metadata": {},
   "source": [
    "We will now define some notation and reformulate the decision rule for our analysis.\n",
    "\n",
    "First, we would like to know the class posteriors $\\mathbb{P}(G|X)$.\n",
    "\n",
    "Let $f_k(x)$ be class-conditional density of X given class $G=k$.\n",
    "\n",
    "Let $\\pi_k$ be the prior probability of belonging to class $k$.\n",
    "\n",
    "Using Bayes' theorem, we obtain:\n",
    "$$\\mathbb{P}(G = k \\mid X = x) = \\frac{f_k(x) \\pi_k}{\\sum_{j=1}^{K} f_j(x) \\pi_j}$$\n",
    "\n",
    "For LDA and QDA, we make the assumption that the class-conditional densities are Gaussian.\n",
    "\n",
    "Hence, we have:\n",
    "\n",
    "$$f_k(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_k)^\\top \\Sigma^{-1} (x - \\mu_k) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838ab3a-311a-45ce-bafc-bc8593a2111c",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "Next, for LDA we further assume that the covariance matrices for each class are equal.\n",
    "\n",
    "$$\\Sigma_k = \\Sigma, \\; \\forall k$$\n",
    "\n",
    "This assumption is vital for LDA and this is what differentiates LDA from QDA.\n",
    "\n",
    "We want to compare two different classes to help to classify our data. We will use the log-odds ratio and see that in LDA, the log-odds results in a linear expression. This is the reason for the name 'Linear Discriminant Analysis'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb6554-22d3-4fec-83a1-07a2f02d406d",
   "metadata": {},
   "source": [
    "We have that:\n",
    "$$\\mathbb{P}(G = k \\mid X = x) = \\frac{f_k(x) \\pi_k}{\\sum_{j=1}^{K} f_j(x) \\pi_j}$$\n",
    "Then, the log-odds is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log \\frac{\\mathbb{P}(G = k \\mid X = x)}{\\mathbb{P}(G = \\ell \\mid X = x)} \n",
    "& = \\log \\frac{f_k(x)}{f_\\ell(x)} + \\log \\frac{\\pi_k}{\\pi_\\ell} \\\\\n",
    "& = \\log \\frac{\\pi_k}{\\pi_\\ell} - \\frac{1}{2} (\\mu_k + \\mu_\\ell)^\\top \\Sigma^{-1} (\\mu_k - \\mu_\\ell) \\\\\n",
    "& \\quad + x^\\top \\Sigma^{-1} (\\mu_k - \\mu_\\ell)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can see that the covariance assumption allowed terms to cancel out, giving a final expression which is linear in $x$. The decision boundary which allows us to distinguish between different classes is the set where the conditional-class probabilities are equal for the different classes. Since the log-odds gives a linear expression, the decision boundary in LDA is a hyperplane.\n",
    "\n",
    "Furthermore, we define the discriminant function in LDA:\n",
    "$$\n",
    "\\delta_k(x) = x^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log \\pi_k\n",
    "$$\n",
    "with $\\Sigma$ the covariance matrix, and $\\mu_k$ the mean and $\\pi_k$ the prior probability for class $k$\n",
    "\n",
    "We can see that the log-odds is the difference $\\delta_k(x) - \\delta_l(x)$.\n",
    "To classify our data optimally, we must maximise the posterior probability in the decision rule for a given $k$. This is equivalent to maximising the the log-odds, which in turn is equivalent to maximising the discriminant function so this is what we aim to do. Hence, we have now reformulated the decision rule to maximising the discriminant function for each class $k$.\n",
    "\n",
    "Clearly, we have a number of parameters here. In practice, these values will be unknwn and we need to estimate these using the training data to define our Gaussian densities. Hence, we estimate the prior probability, the class mean and the class covariance matrices as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_k = \\frac{N_k}{N} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_k = \\frac{\\sum_{g_i = k} x_i}{N_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma} = \\frac{\\sum_{k=1}^{K} \\sum_{g_i = k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top}{N - K}\n",
    "$$\n",
    "\n",
    "The 'hat' notation denotes the parameter estimates, $N_k$ is the number of observations in the training data which correspond to the $k$-th class and $K$ is the number of different classes.\n",
    "\n",
    "To classify datapoints, we label them with the class $k$ which maximises their discriminant function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d06553-058f-49e0-ac9f-0f10d899b728",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis\n",
    "Suppose now that we omit the assumption that the covariance matrices are equal. We can then show by a similar derivation that the discriminant function is now quadratic in $x$.\n",
    "\n",
    "The discriminant function in QDA:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = -\\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (x - \\mu_k)^\\top \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k.\n",
    "$$\n",
    "\n",
    "with $\\mu_k$ the mean, $\\Sigma_k$ the covariance matrix and $\\pi_k$ the prior probability for class $k$.\n",
    "\n",
    "Now, the decision boundary is defined by a quadratic equation due to this chang in discriminant function.\n",
    "\n",
    "We still must estimate the parameters for the Gaussian density, however, we must now estimate the covariance matrix for each class sepearately.\n",
    "\n",
    "We again aim to maximise the discriminant function in QDA to classify our datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2500dc-fc6f-4733-9455-50c3ae608f9d",
   "metadata": {},
   "source": [
    "## Comparison of LDA and QDA\n",
    "One drawback of QDA is that if $p$ is large then this significantly increases the number of parameters and the computational cost. If there are $K$ classes, then in LDA we need the differences in discriminant functions of a chosen class $K$ and the other $K-1$ classes. Each difference requifres $p+1$ parameters, hence, LDA requires $(K-1)(p+1)$ parameters. On the other hand, QDA requires $(K-1)(p(p+3)/2+1)$ parameters since each difference requires $(p(p+3)/2+1)$ parameters.\n",
    "\n",
    "The covariance matrix assumption in LDA can negatively impact model performance, whereas in QDA, the absence of this assumption causes QDA to be less likely to be affected by errors due to this. Furthermore, LDA assumes linearity of the log-odds in $x$ which limits the usefulness of LDA in situations where this relationship is more complex.\n",
    "\n",
    "It is clear that LDA is more useful when the data given supports a linear decision boundary, and QDA is more useful when the data given supports a quadratic decision boundary. These decision boundaries are simple and this is why both techniques are widely used. The bias-variance tradeoff is allowing bias from a simpler decision boundary in order to reduce the variance, since more complex decision boundaries typically have much hugher variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27fef02-b27f-4cb2-bb49-ee8a01a7cb0d",
   "metadata": {},
   "source": [
    "## Regularised Discriminant Analysis\n",
    "\n",
    "In 1989, Friedman developed a method which aimed to give a combination of the benefits of both LDA and QDA within one model. As we discussed above, LDA has a common covariance matrix for all classes and QDA has different covariance matrices for different classes. This technique - regularised discriminant analysis - allows the shrinkage of the distinct covariance matrices in QDA towards the common matrix in LDA.\n",
    "\n",
    "We control the shrinkage with parameter $\\alpha \\in [0, 1]$ s follows:\n",
    "$$\n",
    "\\hat{\\Sigma}_k(\\alpha) = \\alpha \\hat{\\Sigma}_k + (1 - \\alpha) \\hat{\\Sigma}\n",
    "$$\n",
    "with $\\hat{\\Sigma}$ the covariance matrix used in LDA and $\\hat{\\Sigma}_k$ the covariance matrix used in QDA for corresponding $k$.\n",
    "\n",
    "Then, using these new covariance matrices we carry out discriminant analysis as we did before, to obtain decision boundaries and classify our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2571117-6765-4160-b64f-e8723e89cf61",
   "metadata": {},
   "source": [
    "## Evaluation of Discriminant Analysis\n",
    "We now discuss the advantages and drawbacks of discriminant analysis methods in general.\n",
    "### Advantages\n",
    "- Can handle classification problems involving more than two classes, unlike other methods including logistic regression.\n",
    "- Helps to understand group differences by showing which variables are important to classify between groups.\n",
    "- Produces interpretable models.\n",
    "- Permits the use of prior information through prior probabilities.\n",
    "- Efficiently handles datasets with large $p$ (the number of covariates).\n",
    "\n",
    "### Drawbacks\n",
    "- Normality assumptions may be invalid and violating these can impact the overall performance of the model.\n",
    "- Discriminant analysis is not very robust in the presence of outliers.\n",
    "- Highly-correlated covariates will lead to unstable estimates and poor model performance.\n",
    "- Overfitting is a possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab1959-cc95-48ee-92db-4efc45ab7042",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We discussed the mathematical theory behind discriminant analysis techniques, including linear discriminatn analysis, quadratic discriminant analysis and regularised discriminant analysis. Next, we evaluated the use of discriminant analysis as a technique in general. Now we briefly discuss the suitability of discriminant analysis for our classification problem, before implementing the discussed techniques with our income dataset.\n",
    "\n",
    "We are aiming to classify datapoints from our income dataset as either '<=50K' or '>50K'. Reflecting upon the above discussion, discriminant analysis is definitely suitable for our problem, however, its success will need to be evaluated and we must do this for a number of different discriminant classification methods to ensure we are using the optimal method. It would make sense to use LDA, QDA and RDA since we have discussed these.\n",
    "\n",
    "LDA and QDA can be implemented using the scikit-learn machine learning library. RDA is not part of this library so we will create our own 'RegularisedDiscriminantAnalysis' class to classify our datapoints in a similar form to the sciki-learn LinearDiscriminantAnalysis and QuadraticDiscriminantAnalysis functions.\n",
    "\n",
    "We will evaluate our models using metrics which may include accuracy, confusion matrices, precision, recall and F1-score. To conclude, we will compare their overall performance with receiver-operator curves. These metrics and curves will be implemented using scikit-learn also. We will use matplotlib and seaborn to create engaging visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b9153-cdcd-44a1-8d0b-9c8e995f545d",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hastie, Trevor, et al. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. New York: springer, 2009.\n",
    "\n",
    "[2] Research Method Article: https://researchmethod.net/discriminant-analysis/\n",
    "\n",
    "[3] scikit-learn Documentation - 1.2 Linear and Quadratic Discriminant Analysis: https://scikit-learn.org/stable/modules/lda_qda.html\n",
    "\n",
    "[4] scikit-learn Documenation - 6.3 Preprocessing data: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "[5] sckit-learn Documentation - 3.4 Metrics and scoring: https://scikit-learn.org/stable/modules/model_evaluation.html#\n",
    "\n",
    "[5] Sebastian Raschka’s PCA vs LDA article with Python Examples: https://sebastianraschka.com/Articles/2014_python_lda.html#lda-via-scikit-learn\n",
    "\n",
    "[6] Data Science Toolbox Lecture Notes on ROC Curves: https://dsbristol.github.io/dst/assets/slides/05.1-Classification.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
