{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "In this document, we provide an introduction to the mathematics behind classification problems. In classification, we seek to make predictions about the class $Y$ of an object, using features $\\boldsymbol{x}$. \n",
    "\n",
    "Assume that the classes take values in a discrete set $\\mathcal{J}$. Since we make predictions in $\\mathcal{J}$, the use of the squared error from regression is not appropriate as the classes do not even have to be numerical. We therefore introduce a non-negative loss function $L = L(Y, \\widehat{G^*}(\\boldsymbol{x}))$ that measures the loss incurred when predicting $\\widehat{G^*}(\\boldsymbol{x})$ when the true value is $Y$.\n",
    "\n",
    "A natural goal is to find a function $\\widehat{G^*}(\\boldsymbol{x})$ that minimizes the expected loss (risk) $R^*$:\n",
    "\n",
    "$$\n",
    "R^*(Y, \\widehat{G^*}(\\boldsymbol{x})) = \\mathbb{E}_{\\boldsymbol{x}, Y}[L(Y, \\widehat{G^*}(\\boldsymbol{x}))],\n",
    "$$\n",
    "\n",
    "where $L(Y, \\widehat{G^*}(\\boldsymbol{x}))$ is . A popular choice of loss function is 0/1 loss, $L_{0/1}$:\n",
    "\n",
    "$$\n",
    "L_{0/1}(Y, \\widehat{G^*}(\\boldsymbol{x})) = \\mathbb{I}(Y \\neq \\widehat{G^*}(\\boldsymbol{x})),\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}$ is the indicator function, taking the value 1 if the condition is met, and 0 otherwise. The corresponding risk is:\n",
    "\n",
    "$$\n",
    "R^*_{0/1}(Y, \\widehat{G^*}(\\boldsymbol{x})) = \\Pr(Y \\neq \\widehat{G^*}(\\boldsymbol{x})) = 1 - \\Pr(Y = \\widehat{G^*}(\\boldsymbol{x})),\n",
    "$$\n",
    "\n",
    "where $f_j$ is the density of $\\boldsymbol{x}$ conditional on $Y = j$. The optimal classifier minimizing $R^*_{0/1}$ is called the Bayes classifier $\\widehat{G^*} = \\widehat{G^*}_{\\text{Bayes}}(\\boldsymbol{x})$. Using the expression above, we can see that it takes the form:\n",
    "\n",
    "$$\n",
    "\\widehat{G^*}_{\\text{Bayes}}(\\boldsymbol{x}) = \\arg \\min_{j \\in \\mathcal{Y}} R^*_{0/1}(Y, j) = \\arg \\max_{j \\in \\mathcal{Y}} \\Pr(Y=j \\mid \\boldsymbol{x}).\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem, we can write $$\\Pr(Y=j \\mid \\boldsymbol{x}) = \\frac{\\Pr(Y=j)f_j(\\boldsymbol{x})}{\\sum_{k \\in \\mathcal{J}} \\Pr(Y=k)f_k(\\boldsymbol{x})}.$$ The denominator does not depend on $j$, and therefore we have \n",
    "\n",
    "$$\n",
    "\\widehat{G^*}_{\\text{Bayes}}(\\boldsymbol{x}) = \\arg \\max_{j \\in \\mathcal{Y}} \\Pr(Y=j) f_j(\\boldsymbol{x}),\n",
    "$$\n",
    "\n",
    "and after applying a log-transformation:\n",
    "\n",
    "$$\n",
    "\\widehat{G^*}_{\\text{Bayes}}(\\boldsymbol{x}) = \\arg \\max_{j \\in \\mathcal{Y}} \\left\\{ \\log \\Pr(Y=j) + \\log f_j(\\boldsymbol{x}) \\right\\}.\n",
    "$$\n",
    "\n",
    "However, solving this problem to obtain the Bayes classifier is computationally difficult, as the distribution $\\Pr$ and the conditional densities $f_j$ belong to a large class of functions. In practice, we would use a dataset $L = \\{(\\boldsymbol{x}_1, y_1), \\dots, (\\boldsymbol{x}_N, y_N)\\}$ with $N$ data points and attempt to minimize the empirical risk:\n",
    "\n",
    "$$\n",
    "R_{0/1}(Y, \\widehat{G^*}(\\boldsymbol{x})) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(y_i \\neq \\widehat{G^*}(\\boldsymbol{x}_i)).\n",
    "$$\n",
    "\n",
    "Many classification methods aim to approximate the Bayes classifier by minimizing the above risk. In particular, we get parametric methods, which assume a parametric class of functions for $f_j$, and non-parametric methods which do not make any assumptions on $f_j$. In this report, we will look at two parametric methods: discriminant analysis (Section 3) and logistic regression (section 4). We will then look at some non-parametric tree-based methods in Section 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. \"The elements of statistical learning: data mining, inference, and prediction.\" (2017).\n",
    "\n",
    "[2] James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\n",
    "\n",
    "[3] Breiman, Leo. Classification and regression trees. Routledge, 2017."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
